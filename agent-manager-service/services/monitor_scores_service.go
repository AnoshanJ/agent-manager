// Copyright (c) 2026, WSO2 LLC. (https://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

package services

import (
	"errors"
	"fmt"
	"log/slog"
	"time"

	"github.com/google/uuid"
	"gorm.io/gorm"

	"github.com/wso2/ai-agent-management-platform/agent-manager-service/models"
	"github.com/wso2/ai-agent-management-platform/agent-manager-service/repositories"
	"github.com/wso2/ai-agent-management-platform/agent-manager-service/utils"
)

// MonitorScoresService handles evaluation score business logic
type MonitorScoresService struct {
	repo   repositories.ScoreRepository
	logger *slog.Logger
}

// NewMonitorScoresService creates a new monitor scores service
func NewMonitorScoresService(
	repo repositories.ScoreRepository,
	logger *slog.Logger,
) *MonitorScoresService {
	return &MonitorScoresService{
		repo:   repo,
		logger: logger,
	}
}

// PublishScores stores evaluation scores and aggregates for a monitor run
func (s *MonitorScoresService) PublishScores(
	monitorID, runID uuid.UUID,
	req *models.PublishScoresRequest,
) error {
	// Step 1: Build run evaluators (IDs generated by DB via gen_random_uuid())
	runEvaluators := make([]models.MonitorRunEvaluator, len(req.AggregatedScores))
	for i, item := range req.AggregatedScores {
		runEvaluators[i] = models.MonitorRunEvaluator{
			MonitorRunID:  runID,
			MonitorID:     monitorID,
			EvaluatorName: item.Identifier,
			DisplayName:   item.DisplayName,
			Level:         item.Level,
			Aggregations:  item.Aggregations,
			Count:         item.Count,
			SkippedCount:  item.SkippedCount,
		}
	}

	// Step 2: Validate individual scores before starting the transaction
	for _, item := range req.IndividualScores {
		if item.TraceTimestamp == nil {
			return fmt.Errorf("TraceTimestamp is required for score with traceID %s", item.TraceID)
		}
	}

	// Step 3: Upsert evaluators and scores atomically
	if err := s.repo.RunInTransaction(func(txRepo repositories.ScoreRepository) error {
		if err := txRepo.UpsertMonitorRunEvaluators(runEvaluators); err != nil {
			return fmt.Errorf("failed to upsert run evaluators: %w", err)
		}

		// Reload evaluators to get the actual DB-generated IDs (upsert may keep existing IDs on conflict)
		dbEvaluators, err := txRepo.GetEvaluatorsByMonitorAndRunID(monitorID, runID)
		if err != nil {
			return fmt.Errorf("failed to reload run evaluators: %w", err)
		}

		evaluatorMap := make(map[string]uuid.UUID)
		for _, re := range dbEvaluators {
			evaluatorMap[re.DisplayName] = re.ID
		}

		// Collect current run evaluator IDs and trace IDs for stale score cleanup
		currentRunEvaluatorIDs := make([]uuid.UUID, len(dbEvaluators))
		for i, re := range dbEvaluators {
			currentRunEvaluatorIDs[i] = re.ID
		}

		traceIDSet := make(map[string]struct{})
		for _, item := range req.IndividualScores {
			traceIDSet[item.TraceID] = struct{}{}
		}
		traceIDs := make([]string, 0, len(traceIDSet))
		for tid := range traceIDSet {
			traceIDs = append(traceIDs, tid)
		}

		// Delete stale scores from previous runs before inserting new ones
		if err := txRepo.DeleteStaleScores(monitorID, currentRunEvaluatorIDs, traceIDs); err != nil {
			return fmt.Errorf("failed to delete stale scores: %w", err)
		}

		// Build scores using the real DB IDs
		scores := make([]models.Score, len(req.IndividualScores))
		for i, item := range req.IndividualScores {
			runEvaluatorID, exists := evaluatorMap[item.DisplayName]
			if !exists {
				return fmt.Errorf("evaluator %s not found in evaluators list", item.DisplayName)
			}

			scores[i] = models.Score{
				RunEvaluatorID: runEvaluatorID,
				MonitorID:      monitorID,
				TraceID:        item.TraceID,
				SpanID:         item.SpanID,
				Score:          item.Score,
				Explanation:    item.Explanation,
				TraceTimestamp: *item.TraceTimestamp,
				Metadata:       item.Metadata,
				SkipReason:     item.SkipReason,
			}
		}

		if err := txRepo.BatchCreateScores(scores); err != nil {
			return fmt.Errorf("failed to create scores: %w", err)
		}

		return nil
	}); err != nil {
		return err
	}

	s.logger.Info("Published evaluation scores",
		"monitorID", monitorID,
		"runID", runID,
		"scoreCount", len(req.IndividualScores),
		"evaluatorCount", len(runEvaluators))

	return nil
}

// GetMonitorScores returns aggregated scores for a monitor within a time range
func (s *MonitorScoresService) GetMonitorScores(
	monitorID uuid.UUID,
	monitorName string,
	startTime, endTime time.Time,
	filters repositories.ScoreFilters,
) (*models.MonitorScoresResponse, error) {
	// Use SQL aggregation query instead of fetching all rows
	aggregations, err := s.repo.GetMonitorScoresAggregated(monitorID, startTime, endTime, filters)
	if err != nil {
		return nil, fmt.Errorf("failed to get aggregated scores: %w", err)
	}

	// Convert aggregations to response format
	evaluators := make([]models.EvaluatorScoreSummary, len(aggregations))
	for i, agg := range aggregations {
		aggregationMap := make(map[string]interface{})
		if agg.MeanScore != nil {
			aggregationMap["mean"] = *agg.MeanScore
		}

		evaluators[i] = models.EvaluatorScoreSummary{
			EvaluatorName: agg.EvaluatorName,
			Level:         agg.Level,
			Count:         agg.TotalCount,
			SkippedCount:  agg.SkippedCount,
			Aggregations:  aggregationMap,
		}
	}

	return &models.MonitorScoresResponse{
		MonitorName: monitorName,
		TimeRange: models.TimeRange{
			Start: startTime,
			End:   endTime,
		},
		Evaluators: evaluators,
	}, nil
}

// GetEvaluatorTimeSeries returns time-series data for a specific evaluator.
// Granularity is automatically determined based on data density and time range.
func (s *MonitorScoresService) GetEvaluatorTimeSeries(
	monitorID uuid.UUID,
	monitorName, displayName string,
	startTime, endTime time.Time,
) (*models.TimeSeriesResponse, error) {
	// Count data points to determine adaptive granularity
	count, err := s.repo.GetEvaluatorScoreCount(monitorID, displayName, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to count scores for adaptive granularity: %w", err)
	}

	duration := endTime.Sub(startTime)
	granularity := utils.CalculateAdaptiveGranularity(duration, count)

	// Trace-level aggregation for sparse data
	if granularity == "trace" {
		return s.getTraceAggregatedTimeSeries(monitorID, monitorName, displayName, startTime, endTime)
	}

	// Time-bucket aggregation for dense data
	aggregations, err := s.repo.GetEvaluatorTimeSeriesAggregated(monitorID, displayName, startTime, endTime, granularity)
	if err != nil {
		return nil, fmt.Errorf("failed to get time series aggregations: %w", err)
	}

	points := make([]models.TimeSeriesPoint, len(aggregations))
	for i, agg := range aggregations {
		aggregationMap := make(map[string]interface{})
		if agg.MeanScore != nil {
			aggregationMap["mean"] = *agg.MeanScore
		}

		points[i] = models.TimeSeriesPoint{
			Timestamp:    agg.TimeBucket,
			Count:        agg.TotalCount,
			SkippedCount: agg.SkippedCount,
			Aggregations: aggregationMap,
		}
	}

	return &models.TimeSeriesResponse{
		MonitorName:   monitorName,
		EvaluatorName: displayName,
		Granularity:   granularity,
		Points:        points,
	}, nil
}

// getTraceAggregatedTimeSeries returns per-trace aggregated scores as time series points.
// Each trace becomes a single data point with its exact timestamp and mean score.
func (s *MonitorScoresService) getTraceAggregatedTimeSeries(
	monitorID uuid.UUID,
	monitorName, displayName string,
	startTime, endTime time.Time,
) (*models.TimeSeriesResponse, error) {
	traceAggs, err := s.repo.GetEvaluatorTraceAggregated(monitorID, displayName, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to get trace-aggregated scores: %w", err)
	}

	points := make([]models.TimeSeriesPoint, len(traceAggs))
	for i, agg := range traceAggs {
		aggregationMap := make(map[string]interface{})
		if agg.MeanScore != nil {
			aggregationMap["mean"] = *agg.MeanScore
		}

		points[i] = models.TimeSeriesPoint{
			Timestamp:    agg.TraceTimestamp,
			Count:        agg.TotalCount,
			SkippedCount: agg.SkippedCount,
			Aggregations: aggregationMap,
		}
	}

	return &models.TimeSeriesResponse{
		MonitorName:   monitorName,
		EvaluatorName: displayName,
		Granularity:   "trace",
		Points:        points,
	}, nil
}

// GetTraceScores returns all evaluation scores for a specific trace across all monitors
func (s *MonitorScoresService) GetTraceScores(
	traceID, orgName, projName, agentName string,
) (*models.TraceScoresResponse, error) {
	scores, err := s.repo.GetScoresByTraceID(traceID, orgName, projName, agentName)
	if err != nil {
		return nil, fmt.Errorf("failed to get trace scores: %w", err)
	}

	// Group by monitor â†’ evaluator
	type monitorGroup struct {
		monitorID   uuid.UUID
		monitorName string
		runID       uuid.UUID
		evaluators  map[string]*models.EvaluatorTraceGroup
		evalOrder   []string
	}

	monitorMap := make(map[uuid.UUID]*monitorGroup)
	var monitorOrder []uuid.UUID

	for _, score := range scores {
		mg, exists := monitorMap[score.MonitorID]
		if !exists {
			mg = &monitorGroup{
				monitorID:   score.MonitorID,
				monitorName: score.MonitorName,
				runID:       score.RunID,
				evaluators:  make(map[string]*models.EvaluatorTraceGroup),
				evalOrder:   []string{},
			}
			monitorMap[score.MonitorID] = mg
			monitorOrder = append(monitorOrder, score.MonitorID)
		}

		eg, exists := mg.evaluators[score.EvaluatorName]
		if !exists {
			eg = &models.EvaluatorTraceGroup{
				EvaluatorName: score.EvaluatorName,
				Level:         score.Level,
				Scores:        []models.ScoreItem{},
			}
			mg.evaluators[score.EvaluatorName] = eg
			mg.evalOrder = append(mg.evalOrder, score.EvaluatorName)
		}

		eg.Scores = append(eg.Scores, models.ScoreItem{
			SpanID:      score.SpanID,
			Score:       score.Score,
			Explanation: score.Explanation,
			Metadata:    score.Metadata,
			SkipReason:  score.SkipReason,
		})
	}

	// Build response
	monitors := make([]models.MonitorTraceGroup, len(monitorOrder))
	for i, monitorID := range monitorOrder {
		mg := monitorMap[monitorID]

		evaluators := make([]models.EvaluatorTraceGroup, len(mg.evalOrder))
		for j, evalName := range mg.evalOrder {
			evaluators[j] = *mg.evaluators[evalName]
		}

		monitors[i] = models.MonitorTraceGroup{
			MonitorName: mg.monitorName,
			MonitorID:   mg.monitorID.String(),
			RunID:       mg.runID.String(),
			Evaluators:  evaluators,
		}
	}

	return &models.TraceScoresResponse{
		TraceID:  traceID,
		Monitors: monitors,
	}, nil
}

// GetMonitorRunScores returns per-run aggregated scores from the MonitorRunEvaluator records.
func (s *MonitorScoresService) GetMonitorRunScores(
	monitorID uuid.UUID,
	runID uuid.UUID,
	monitorName string,
) (*models.MonitorRunScoresResponse, error) {
	evaluators, err := s.repo.GetEvaluatorsByMonitorAndRunID(monitorID, runID)
	if err != nil {
		return nil, fmt.Errorf("failed to get run evaluators: %w", err)
	}

	summaries := make([]models.EvaluatorScoreSummary, len(evaluators))
	for i, eval := range evaluators {
		aggs := eval.Aggregations
		if aggs == nil {
			aggs = make(map[string]interface{})
		}
		summaries[i] = models.EvaluatorScoreSummary{
			EvaluatorName: eval.DisplayName,
			Level:         eval.Level,
			Count:         eval.Count,
			SkippedCount:  eval.SkippedCount,
			Aggregations:  aggs,
		}
	}

	return &models.MonitorRunScoresResponse{
		RunID:       runID.String(),
		MonitorName: monitorName,
		Evaluators:  summaries,
	}, nil
}

// GetMonitorID resolves monitor name to monitor ID
func (s *MonitorScoresService) GetMonitorID(orgName, projName, agentName, monitorName string) (uuid.UUID, error) {
	id, err := s.repo.GetMonitorID(orgName, projName, agentName, monitorName)
	if err != nil {
		if errors.Is(err, gorm.ErrRecordNotFound) {
			return uuid.Nil, fmt.Errorf("monitor %q not found: %w", monitorName, utils.ErrMonitorNotFound)
		}
		return uuid.Nil, fmt.Errorf("failed to query monitor: %w", err)
	}
	return id, nil
}
