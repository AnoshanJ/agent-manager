// Code generated by scripts/generate-builtin-evaluators.sh; DO NOT EDIT.

package catalog

import "github.com/wso2/ai-agent-management-platform/agent-manager-service/models"

var entries = []*Entry{
	{
		Identifier:  "answer_length",
		DisplayName: "Answer Length",
		Description: "Checks output character length falls within configured min/max bounds. Scores 1.0 if within range, 0.0 otherwise.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "AnswerLengthEvaluator",
		Tags:        []string{"builtin", "rule-based", "quality"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "max_length", Type: "integer", Description: "Maximum acceptable length", Required: false, Default: float64(10000), Min: floatPtr(1)},
			{Key: "min_length", Type: "integer", Description: "Minimum acceptable length", Required: false, Default: float64(1), Min: floatPtr(0)},
		},
	},
	{
		Identifier:  "iteration_count",
		DisplayName: "Iteration Count",
		Description: "Checks total span count against a configurable max. Scores 1.0 within limit, degrades linearly above it.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "IterationCountEvaluator",
		Tags:        []string{"builtin", "rule-based", "efficiency"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "max_iterations", Type: "integer", Description: "Maximum allowed iterations", Required: false, Default: float64(10), Min: floatPtr(1)},
			{Key: "use_context_constraint", Type: "boolean", Description: "Whether to use task.constraints.max_iterations", Required: false, Default: true},
		},
	},
	{
		Identifier:  "latency",
		DisplayName: "Latency",
		Description: "Checks total execution time against a configurable limit. Scores 1.0 within limit, degrades linearly above it.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "LatencyEvaluator",
		Tags:        []string{"builtin", "rule-based", "efficiency"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "max_latency_ms", Type: "float", Description: "Maximum allowed latency in milliseconds", Required: false, Default: float64(30000.0), Min: floatPtr(0.0)},
			{Key: "use_task_constraint", Type: "boolean", Description: "Whether to use task.constraints.max_latency_ms", Required: false, Default: true},
		},
	},
	{
		Identifier:  "prohibited_content",
		DisplayName: "Prohibited Content",
		Description: "Flags output containing any prohibited strings or patterns. Scores 0.0 if any match found, 1.0 if clean. Also reads task.prohibited_content when available.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "ProhibitedContentEvaluator",
		Tags:        []string{"builtin", "rule-based", "safety", "compliance"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "case_sensitive", Type: "boolean", Description: "Whether to use case-sensitive matching", Required: false, Default: false},
			{Key: "prohibited_patterns", Type: "string", Description: "List of prohibited regex patterns", Required: false, Default: nil},
			{Key: "prohibited_strings", Type: "string", Description: "List of prohibited strings", Required: false, Default: nil},
			{Key: "use_context_prohibited", Type: "boolean", Description: "Whether to use task.prohibited_content", Required: false, Default: true},
		},
	},
	{
		Identifier:  "required_content",
		DisplayName: "Required Content",
		Description: "Checks output contains all required strings and regex patterns. Score = proportion of required items found.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "RequiredContentEvaluator",
		Tags:        []string{"builtin", "rule-based", "compliance"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "case_sensitive", Type: "boolean", Description: "Whether to use case-sensitive matching", Required: false, Default: false},
			{Key: "required_patterns", Type: "string", Description: "List of required regex patterns", Required: false, Default: nil},
			{Key: "required_strings", Type: "string", Description: "List of required strings", Required: false, Default: nil},
		},
	},
	{
		Identifier:  "required_tools",
		DisplayName: "Required Tools",
		Description: "Confirms all required tools were invoked at least once. Score = proportion of required tools found.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "RequiredToolsEvaluator",
		Tags:        []string{"builtin", "rule-based", "tool-use"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "required_tools", Type: "string", Description: "Set of required tool names", Required: false, Default: nil},
		},
	},
	{
		Identifier:  "step_success_rate",
		DisplayName: "Step Success Rate",
		Description: "Measures the ratio of execution spans completed without errors. Score = successful spans / total spans.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "StepSuccessRateEvaluator",
		Tags:        []string{"builtin", "rule-based", "tool-use"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "min_success_rate", Type: "float", Description: "Minimum required success rate", Required: false, Default: float64(0.8), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "token_efficiency",
		DisplayName: "Token Efficiency",
		Description: "Checks total token usage against a configurable limit. Scores 1.0 within limit, degrades linearly above it.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "TokenEfficiencyEvaluator",
		Tags:        []string{"builtin", "rule-based", "efficiency"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "max_tokens", Type: "integer", Description: "Maximum allowed tokens", Required: false, Default: float64(10000), Min: floatPtr(1)},
			{Key: "use_context_constraint", Type: "boolean", Description: "Whether to use task.constraints.max_tokens", Required: false, Default: true},
		},
	},
	{
		Identifier:  "tool_sequence",
		DisplayName: "Tool Sequence",
		Description: "Verifies tools were invoked in the expected order. In non-strict mode, allows extra tools between expected ones. Score = proportion of sequence matched.",
		Version:     "1.0",
		Provider:    "standard",
		ClassName:   "ToolSequenceEvaluator",
		Tags:        []string{"builtin", "rule-based", "tool-use"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "expected_sequence", Type: "string", Description: "List of tool names in expected order", Required: false, Default: nil},
			{Key: "strict", Type: "boolean", Description: "If True, requires exact sequence. If False, allows extra tools", Required: false, Default: false},
			{Key: "use_context_trajectory", Type: "boolean", Description: "If True, uses task.expected_trajectory", Required: false, Default: true},
		},
	},
	{
		Identifier:  "accuracy",
		DisplayName: "Accuracy",
		Description: "Scores factual correctness of information in the response. Works without evidence (unlike faithfulness which requires tool/retrieval data).",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "AccuracyEvaluator",
		Tags:        []string{"builtin", "llm-judge", "correctness"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "clarity",
		DisplayName: "Clarity",
		Description: "Scores the response for readability, structure, and absence of ambiguity. Checks whether the detail level matches the user's apparent expertise.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "ClarityEvaluator",
		Tags:        []string{"builtin", "llm-judge", "quality"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "coherence",
		DisplayName: "Coherence",
		Description: "Scores each LLM call for logical flow, internal consistency, and structure. Runs per LLM span, catching incoherent reasoning in intermediate steps.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "CoherenceEvaluator",
		Tags:        []string{"builtin", "llm-judge", "quality"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "completeness",
		DisplayName: "Completeness",
		Description: "Checks whether the final response addresses all sub-questions and requirements in the input. Accepts optional success_criteria. 0.0 = nothing addressed, 1.0 = fully covered.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "CompletenessEvaluator",
		Tags:        []string{"builtin", "llm-judge", "quality"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "conciseness",
		DisplayName: "Conciseness",
		Description: "Scores each LLM call for unnecessary verbosity and filler phrases. Does not penalize thoroughness, only padding. Runs per LLM span.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "ConcisenessEvaluator",
		Tags:        []string{"builtin", "llm-judge", "quality", "efficiency"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "context_relevance",
		DisplayName: "Context Relevance",
		Description: "Scores whether documents retrieved by RAG pipelines are relevant to the user's query. Skips traces with no retrieval spans (configurable via on_missing_context).",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "ContextRelevanceEvaluator",
		Tags:        []string{"builtin", "llm-judge", "relevance"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "on_missing_context", Type: "string", Description: "Behavior when no retrieval spans are found: 'skip' returns EvalResult.skip(), 'zero' returns score=0.0", Required: false, Default: "skip", EnumValues: []string{"skip", "zero"}},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "error_recovery",
		DisplayName: "Error Recovery",
		Description: "Scores how gracefully the agent detects and recovers from errors during execution. Skips traces with no errors by default. Runs per agent.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "ErrorRecoveryEvaluator",
		Tags:        []string{"builtin", "llm-judge", "reasoning"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "on_missing_context", Type: "string", Description: "Behavior when no errors are found in the agent trace: 'skip' returns EvalResult.skip(), 'zero' returns score=0.0", Required: false, Default: "skip", EnumValues: []string{"skip", "zero"}},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "faithfulness",
		DisplayName: "Faithfulness",
		Description: "Verifies that factual claims in the response are grounded in tool results and retrieved documents. Skips traces with no tool or retrieval data (configurable via on_missing_context).",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "FaithfulnessEvaluator",
		Tags:        []string{"builtin", "llm-judge", "correctness"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "on_missing_context", Type: "string", Description: "Behavior when no tool or retrieval spans are found: 'skip' returns EvalResult.skip(), 'zero' returns score=0.0", Required: false, Default: "skip", EnumValues: []string{"skip", "zero"}},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "goal_clarity",
		DisplayName: "Goal Clarity",
		Description: "Scores whether the agent demonstrates clear understanding of the user's goal in its first response or planning step. Runs per agent.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "GoalClarityEvaluator",
		Tags:        []string{"builtin", "llm-judge", "reasoning"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "hallucination",
		DisplayName: "Hallucination",
		Description: "Detects fabricated facts, invented statistics, and unsupported claims in the response. When tool or retrieval evidence is available, verifies claims against gathered evidence. 0.0 = significant hallucinations, 1.0 = fully grounded.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "HallucinationEvaluator",
		Tags:        []string{"builtin", "llm-judge", "correctness", "safety"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.7), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "helpfulness",
		DisplayName: "Helpfulness",
		Description: "Scores whether the response actually helps the user with what they asked for. Checks for actionable, useful content vs empty acknowledgments.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "HelpfulnessEvaluator",
		Tags:        []string{"builtin", "llm-judge", "quality"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "instruction_following",
		DisplayName: "Instruction Following",
		Description: "Checks whether the agent follows instructions from system prompts or task descriptions. Reads system prompts from agent and LLM spans. Skips when no instructions are found (configurable).",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "InstructionFollowingEvaluator",
		Tags:        []string{"builtin", "llm-judge", "compliance"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "on_missing_context", Type: "string", Description: "Behavior when no system prompt or task instructions are found: 'skip' returns EvalResult.skip(), 'zero' returns score=0.0", Required: false, Default: "skip", EnumValues: []string{"skip", "zero"}},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "path_efficiency",
		DisplayName: "Path Efficiency",
		Description: "Scores whether the agent's execution path is efficient. Detects redundant steps, loops, and wasted work. Runs per agent.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "PathEfficiencyEvaluator",
		Tags:        []string{"builtin", "llm-judge", "efficiency"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "reasoning_quality",
		DisplayName: "Reasoning Quality",
		Description: "Scores whether the agent's execution steps are logical, purposeful, and well-reasoned. Runs per agent.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "ReasoningQualityEvaluator",
		Tags:        []string{"builtin", "llm-judge", "reasoning"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "relevance",
		DisplayName: "Relevance",
		Description: "Scores whether the final response is semantically relevant to the user's query, accounting for paraphrasing and synonyms.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "RelevanceEvaluator",
		Tags:        []string{"builtin", "llm-judge", "relevance"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "safety",
		DisplayName: "Safety",
		Description: "Checks each LLM call for harmful, toxic, biased, or policy-violating content across 8 categories. Runs per LLM span. Accepts optional context param for domain-specific rules. 0.0 = unsafe, 1.0 = safe.",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "SafetyEvaluator",
		Tags:        []string{"builtin", "llm-judge", "safety"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "context", Type: "string", Description: "Optional context about the interaction type (e.g., 'customer support', 'medical advice', 'children's education')", Required: false, Default: ""},
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.7), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
	{
		Identifier:  "tone",
		DisplayName: "Tone",
		Description: "Scores each LLM call for appropriate and professional tone. Runs per LLM span. Accepts optional context param (e.g. 'customer support', 'technical docs').",
		Version:     "1.0",
		Provider:    "llm_judge",
		ClassName:   "ToneEvaluator",
		Tags:        []string{"builtin", "llm-judge", "quality"},
		ConfigSchema: []models.EvaluatorConfigParam{
			{Key: "context", Type: "string", Description: "Optional context about the expected tone (e.g., 'customer support', 'technical documentation', 'casual chat')", Required: false, Default: ""},
			{Key: "criteria", Type: "string", Description: "Evaluation criteria", Required: false, Default: "quality, accuracy, and helpfulness"},
			{Key: "max_retries", Type: "integer", Description: "Max retries on invalid LLM output", Required: false, Default: float64(2)},
			{Key: "max_tokens", Type: "integer", Description: "Max tokens for LLM response", Required: false, Default: float64(1024)},
			{Key: "model", Type: "string", Description: "LLM model identifier", Required: false, Default: "gpt-4o-mini"},
			{Key: "temperature", Type: "float", Description: "LLM temperature", Required: false, Default: float64(0.0)},
			{Key: "threshold", Type: "float", Description: "Pass threshold (0.0-1.0)", Required: false, Default: float64(0.5), Min: floatPtr(0.0), Max: floatPtr(1.0)},
		},
	},
}
