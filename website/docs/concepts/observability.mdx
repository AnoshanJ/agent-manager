---
sidebar_position: 1
---

# Observability

WSO2 Agent Manager provides full-stack observability for AI agents — whether they are deployed through the platform or running externally. Traces, metrics, and logs flow into a centralized store that you can query and analyze through the AMP Console.

## Overview

Observability in AMP is built on [OpenTelemetry](https://opentelemetry.io/), the industry-standard framework for distributed tracing and instrumentation. Every agent interaction — LLM calls, tool invocations, MCP requests, retrieval operations, and agent reasoning steps is captured as a structured trace and stored for analysis.

## Auto-Instrumentation for Deployed Agents

When you deploy an agent through WSO2 Agent Manager, observability is set up **automatically — no code changes required**.

### What Gets Instrumented

The Traceloop SDK (used under the hood) instruments a wide range of AI frameworks automatically:

| Category | Examples |
|----------|---------|
| LLM providers | OpenAI, Anthropic, Azure OpenAI |
| Agent frameworks | LangChain, LlamaIndex, CrewAI, Haystack |
| Vector stores | Pinecone, Weaviate, Chroma, Qdrant |
| MCP clients | Any MCP tool calls made by the agent |

### Trace Attributes Captured

Each span is enriched with metadata that makes it possible to evaluate and debug agent behaviour:

- **LLM spans**: model name, prompt tokens, completion tokens, latency, finish reason
- **Tool spans**: tool name, input arguments, output, execution time
- **Agent spans**: agent name, step number, reasoning output
- **Root span**: agent ID, deployment ID, correlation ID, end-to-end latency

## Observability for External Agents

Agents that are **not deployed through AMP** — for example, agents running locally, on-premises, in a third-party cloud can still send traces to AMP using the `amp-instrumentation` package.

## Trace Visibility in AMP Console

Once traces start flowing in, you can explore them in the AMP Console
